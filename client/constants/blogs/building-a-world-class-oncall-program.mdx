---
title: "Building a World-Class Oncall Program"
description: "With careful preparation, you can build a best-in-class software engineering oncall program with great training, runbooks, and prepared engineers."
imageUrl: "/images/blog/building-a-world-class-oncall-program/fireman.webp"
dateWritten: "2023-03-26"
isDraft: false
tags:
  - Processes
---

<img alt="" src="/images/blog/building-a-world-class-oncall-program/fireman.webp" title="" height="908" width="1520">

# Building a World-Class Oncall Program

> Many of the ideas here are taken from my experience (and mistakes) leading the oncall program for Twitter's web app at scale. Please feel free to disagree with this guide in certain places (which is good, as you should always question processes and try to find ways to improve them). However, you can't say that I don't know what I'm talking about.
>
> I will likely tweak the processes here throughout my career and use it as the baseline for oncall programs at my future companies.

## Why is Everything on Fire?

If you're taking the time to read this, it probably means that you've been scorched a time or two by the fire of leading or participating in really tough incidents at work.

Perhaps your team has a good oncall program and you're looking to improve it. Perhaps your team doesn't even have a dedicated oncall program yet.

Whatever the case, you're here for a reason. I'll make it worth your while.

In 2021, I was put in charge of leading the oncall program for all of the Twitter web app (a ~30 engineer rotation at the time pulled from a team of ~120). Our engineers were oncall roughly 2 times a year in both primary and secondary rotations.

Our engineers faced regressions from new feature or infra launches, problems with internal microservices that challenged the partition tolerance of the application, and even wild curveballs like a DDoS attack out of mainland China (which is a story for another time).

Above all, they had to "keep the ship pointed forward in the storm" no matter what so the web app was available for any of the hundreds of millions of active users every day who wanted to use it.

To say this was stressful for engineers while oncall is an understatement.

In the following sections, I'll try to distill insights on how to prevent burnout for oncall engineers, provide good support, educate with proper onboarding, and build a culture that makes sure your team is never caught flat footed during an incident.

Let's get to it. ðŸ”¥

## Oncall Philosophy

You might be surprised to see that this first section is more of an abstract discussion about _what_ oncall even is. But some organizations seem to muddle what oncall should be responsible for and what their tasks should be.

As a general rule, your oncall rotation should either be domain- or team-specific. There needs to be clear boundaries between what they are (and are not) responsible for.

Once those rules are established, it's important to make clear that "the buck stops with oncall" when it comes to emergencies. The oncall gets to make the call about if a rollback (deploying a prior version of the code), revert (reverting a problematic pull request), forward fix (deploying with a revert/code fix instead of a rollback), or other action is necessary. The only time this should be overruled is if management or executives want to make a decision on an incident.

In other words, your primary oncall is the captain in the room in any incident (I'll explain more on the concept of primary and secondary oncalls in a bit). One of the worst feelings as oncall is being responsible for everything (getting pages in the middle of the night) but not _really_ being empowered to make final decisions on any of it. It creates a feeling of helplessness and resentment that will weigh on your oncall program like a stone in water.

In addition to leading incidents and making decisions on them, oncall should be responsible for the following tasks:

- Leading _all_ deploys on the team for the week
- Handling pages for the week (and it should be a big deal if these are missed)
- Analyzing health trends on metrics/logging dashboards for the service(s) they're responsible for
- Triaging new errors that are reported by the support team or error monitoring services
- Creating (and working on) tickets for new errors or oncall-related tasks
- Training new oncall engineers who are shadowing the rotation
- Writing all postmortem documents for any incidents they lead

This seems like a lot, and that's because it is. Some of these tasks can be divvied up between primary and secondary oncalls. What are those, you might ask?

Good question. Let me explain right now.

## Primaries and Secondaries

I've previously said that "the buck stops with oncall." However, what I really meant is that the buck stops with the _primary_ oncall.

If you have a small engineering team (5-6 people), disregard everything that this section has to say (although I'll have a lot more to talk about with scope versus number of rotations soon). If this is you, your rotation is already so small that having a staggered primary/secondary oncall program would _increase_ engineer burnout.

Got it?

Okay. If you're still here, that means you should build two rotations on your team. Each rotation will have the same engineers in it, but they should be staggered (so that an engineer is primary oncall and then maybe secondary half a month later).

Primary oncall should get pages first, lead incidents, be point on training new oncall, and generally be the one in charge of oncall for the entire week.

Secondary oncall should be back up (both with pages if primary misses them as well as with covering oncall if the primary has to be out). You can also choose to divide tasks like writing postmortem documents, traiging some errors, and other lower priority tasks to the secondary oncall.

Primaries should be expected to be within 15 minutes of their computer at _all times_ in case they receive a page. Missing a page should be a **big deal** (and multiple instances of it should be brought up by the manager). As I've said, the buck stops with oncall. This is true of authority as well as responsibility.

Secondaries should be within 15-30 minutes of their latop (unless they let their primary know they'll be out) in case something happens and a page gets escalated to them.

You should find some service that can page the oncall engineers. [PagerDuty](https://www.pagerduty.com/) is popular at time of writing, but it can be expensive - there are other services out there like it, though. These apps should be given override ability on the engineers' phones so oncall engineers _know_ when a problem is going down and they're being paged.

No matter what you do, these pages (which can come at any time of the day or night) _will_ wear on your oncall engineers. That's why you have to be very rigorous about the way you put together a schedule for your oncall program.

## Building a Good Schedule

As I mentioned above, you should make every attempt to place your engineers into a staggered primary/secondary schedule.

Let's dig into this scheduling process a bit more.

In an _ideal_ world, your oncall rotation should be a minimum of 8 people and a maximum of around 25. The reason for this is simple - 8 engineers is a primary oncall shift of around once every 2 months, and 25 engineers is an primary oncall shift around twice a year.

Any less than 8 engineers and you'll be a _big_ risk of burning your oncall engineers out from being oncall all the time (and believe me, feeling like the world is on your shoulders multiple times a month is not fun); any more than 25 and you run the risk of having your engineers forget _how_ to perform their oncall duties (although this can be mitigated with a strong secondary rotation where the secondary oncall engineer actually performs some duties).

Engineers who perform their primary oncall rotation duties should have a week or two off (at minimum) before they do their secondary oncall shift. Having someone oncall for two weeks in a row (primary and then secondary) where they're tied down to their laptop is fairly cruel. This can (rightfully) lead to anger, frustration, and even turnover if it continues despite complaints.

Instead, add new engineers to the rotation at least several months out and give the entire team a heads up publicly when you do it. Tell people to review their future shifts _immediately_ and to let you know if their newly scheduled times (almost a quarter out) don't work. In this instance, the right thing to do is be the one to find them coverage.

Speaking of finding coverage on your schedule, oncall engineers should be responsible at all _other_ times when they need swaps for weeks they can't work their oncall shifts. It should be a **big deal** if someone goes out on vacation and miss getting coverage for their shift.

The final piece of this scheduling puzzle is to do with trainings; you should make a third (and rarely-used) rotation for shadowing engineers. This component needs its own section (further below), so I'll end this one by saying that no team is complete without being people being simultaneously teachers and learners.

## More Scope or More Rotations

Before we get into a discussion of training your team, I'd like to momentarily discuss the tension with oncall schedules between being oncall for more scope and being oncall more frequently.

If you choose more scope, you'll be able to have a large roster of engineers who will be oncall less frequently. This would be on the high end of the roster size (>= 25 people) that I mentioned earlier and would equate to less oncall burnout and engineers not being tied down to their laptops 24/7 a lot of weeks every year (which can build frustration). However, the cons to this approach are that engineers may complain they're losing knowledge on how to properly perform their oncall duties given the large gap in between rotations.

In contrast, having a smaller rotation with much less scope will mean that your engineers get intimately familiar with the subject matter area they're responsible for and run no risk of forgetting it. They will also experience less stress from being responsible for covering a smaller domain while oncall. The main con for this approach is that your engineers will also be oncall more often; this could lead to burnout and prevent them from taking vacations or decompressing when needed from their normal (non-oncall) work.

You should avoid having _both_ large scope and frequent oncall (although you may be forced into it - startups and big corporations will have to make very different trade-offs).

It's important to note that you will **never** reach perfection here. You will always have people who would prefer more rotations and people who would rather have more scope. The key is to kick a direction and stick strongly to it. On top of that, you may build up your rotation only to have some unexpected turnover that momentarily decreases the size of your rotation.

Perfection is not possible; instead, focus on the moving target I just described and do your best to design your rotation (more scope, more rotations, etc.) for what the _current_ needs of your company dictate.

The important part is to pick a direction with your team and stick to it (at least until it becomes obvious a change is needed).

## How to Train Your Team

I've previously mentioned that you should create a "shadow" rotation. Let's expound on that (and other training processes) right now.

I would recommend not adding an engineer to your oncall rotation unless they have met the following qualifications (roughly):

- They are a senior (or higher) engineer and have been on the team for >= 3 months
- They are a junior/mid-level engineer and have been on the team for >= 6 months

This allows these new team members to get used to the codebase, the way your architecture is set up, and the organization itself. However (and as I've mentioned all along), this will need to be shorter if your team is quite small.

Once you've deemed an engineer ready to go into the rotation, you should either meet with them in person/over video chat or have some training materials ready that do the following:

1. Give a breakdown of the philosophy of oncall at your company
2. Describe what primary and secondary oncall are at your company
3. Describe what the tasks are for engineers while they're oncall (and the breakdowns for primary/secondary)

The engineer should then immediately go into the shadow rotation. You should link them up with the primary engineers for the weeks they will be shadowing and make introductions (if they don't already know each other).

I would recommend you put your new oncall engineer through 2 weeks of the shadow rotation. In the first week, they should merely shadow and be a fly on the wall in all incidents, investigations, deploys, rollbacks, and anything else the primary engineer does.

In the second week of shadowing, you should flip the schedule so the shadowing engineer is "primary" from 9-5 pm their local time. They should receive all pages and otherwise function as a primary engineer, but they should have the _full_ support of the training engineer (the "real" primay engineer) in case they have questions. You will need to temporarily modify the paging schedule for this week so the shadowing engineer receives pages during 9-5 pm their local time.

Once an engineer has completed this training, you should view them as ready to go into the full rotation. Place them in the primary and secondary schedules while respecting the issues with adjusting a schedule that I mentioned previously.

## The Art of Redirecting Lightning

As you bring on new engineers, it will become more-and-more important to have a culture where oncall engineers are properly supported by their team.

In the TV show [Avatar: The Last Airbender](https://en.wikipedia.org/wiki/Avatar:_The_Last_Airbender), there is a very wise character named Uncle Iroh. One of the surprising skills he reveals during the show is that he can redirect lightning that is fired at him.

<img alt="Uncle Iroh from the Last Airbender cartoon series bending lightning to his will" height="283" src="/images/blog/building-a-world-class-oncall-program/iroh-lightning.gif" width="380">

Your team should view the oncall engineers (and particularly the primary oncall for each week) as mainly responsible for redirecting lightning in times of crisis. This means that they find the right fix, the right people to work on it, and the right short-term mitigation techniques for an issue. The buck does stop with oncall, but the team needs to be ready to chip in when required. In other words, develop a culture of "it's good to chip in to help oncall."

If oncall is stumped (since no one can possible know everything about this field), a high priority page may warrant manually paging in other key team members for fixing a problem that the oncall engineers have not been able to isolate. This should only be used under extreme circumstances.

However, oncall should always be able to pull in whatever resources they need to get a job done (depending on the severity of the problem).

## Constructing Runbooks

Everybody forgets things, so it's crucial to develop decent runbooks that are _easy_ to flip through in the heat of the moment\_

These should include at a minimum:

1. How to deploy
2. How to rollback
3. Most basic manual processes for the above
4. Leads on teams and the channels associated with those teams
