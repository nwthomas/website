---
title: "Building a World-Class Oncall Program"
description: "With careful preparation, you can build a best-in-class software engineering oncall program with great training, runbooks, and prepared engineers."
imageUrl: "/images/blog/building-a-world-class-oncall-program/fireman.webp"
dateWritten: "2023-03-26"

isDraft: false
tags:
  - Processes
---

<img alt="" src="/images/blog/building-a-world-class-oncall-program/fireman.webp" title="" height="908" width="1520">

# Building a World-Class Oncall Program

> Many of the ideas here are taken from my experience (and mistakes) leading the oncall program for Twitter's web app at scale. Please feel free to disagree with this guide in certain places (which is good, as you should always question processes and try to find ways to improve them). However, you can't say that I don't know what I'm talking about.
>
> I will likely tweak the processes here throughout my career and use it as the baseline for oncall programs at my future companies.

## Why is Everything on Fire?

If you're taking the time to read this, it probably means that you've been scorched a time or two by the fire of leading or participating in really tough incidents at work.

Perhaps your team has a good oncall program and you're looking to improve it. Perhaps your team doesn't even have a dedicated oncall program yet.

Whatever the case, you're here for a reason. I'll make it worth your while.

In 2021, I was put in charge of leading the oncall program for all of the Twitter web app (a ~30 engineer rotation at the time pulled from a team of ~120). Our engineers were oncall roughly 2 times a year in both primary and secondary rotations.

Our engineers faced regressions from new feature or infra launches, problems with internal microservices that challenges the partition tolerance of the application, and even DDoS attacks out of mainland China (which is a story for another time).

Above all, they had to "keep the ship pointed forward in the storm" no matter what so the web app was available for any of the hundreds of millions of active users every day who wanted to use it.

To say this was stressful for engineers while oncall is an understatement.

In the following sections, I'll try to distill insights on how to prevent burnout for oncall engineers, provide good support, educate with proper onboarding, and build a culture that makes sure your team is never caught flat footed during an incident.

Let's get to it. ðŸ”¥

## Oncall Philosophy

You might be surprised to see that this first section is more of an abstract discussion about _what_ oncall even is. But some organizations seem to muddle what oncall should be responsible for and what their tasks should be.

As a general rule, your oncall rotation should either be domain- or team-specific. There needs to be clear boundaries between what they are (and are not) responsible for.

Once those rules are established, it's important to make clear that "the buck stops with oncall" when it comes to emergencies. The oncall gets to make the call about if a rollback (deploying a prior version of the code), revert (reverting a problematic pull request), forward fix (deploying with a revert/code fix instead of a rollback), or other action is necessary. The only time this should be overruled is if management or executives want to make a decision on an incident.

In other words, your primary oncall is the captain in the room in any incident (I'll explain more on the concept of primary and secondary oncalls in a bit). One of the worst feelings as oncall is being responsible for everything (getting pages in the middle of the night) but not _really_ being empowered to make final decisions on any of it. It creates a feeling of helplessness and resentment that will weigh on your oncall program like a stone in water.

In addition to leading incidents and making decisions on them, oncall should be responsible for the following tasks:

- Leading _all_ deploys on the team for the week
- Handling pages for the week (and it should be a big deal if these are missed)
- Analyzing health trends on metrics/logging dashboards for the service(s) they're responsible for
- Triaging new errors that are reported by the support team or error monitoring services
- Creating (and working on) tickets for new errors or oncall-related tasks
- Training new oncall engineers who are shadowing the rotation
- Writing all postmortem documents for any incidents they lead

This seems like a lot, and that's because it is. Some of these tasks can be divvied up between primary and secondary oncalls. What are those, you might ask?

Good question. Let me explain right now.

## Primaries and Secondaries

I've previously said that "the buck stops with oncall." However, what I really meant is that the buck stops with the _primary_ oncall.

If you have a small engineering team (5-6 people), disregard everything that this section has to say. Your rotation is already so small that having a staggered primary/secondary oncall program would _increase_ engineer burnout.

Got it?

Okay. If you're still here, that means you should build two rotations on your team. Each rotation will have the same engineers in it, but they should be staggered (so that an engineer is primary oncall and then maybe secondary half a month later).

Primary oncall should get pages first, lead incidents, be point on training new oncall, and generally be the one in charge of oncall for the entire week.

Secondary oncall should be back up (both with pages if primary misses them as well as with covering oncall if the primary has to be out). You can also choose to divide tasks like writing postmortem documents, traiging some errors, and other lower priority tasks to the secondary oncall.

Primaries should be expected to be within 15 minutes of their computer at _all times_ in case they receive a page. Missing a page should be a **big deal** (and multiple instances of it should be brought up by the manager). As I've said, the buck stops with oncall. This is true of authority as well as responsibility.

Secondaries should be within 15-30 minutes of their latop (unless they let their primary know they'll be out) in case something happens and a page gets escalated to them.

You should find some service that can page the oncall engineers. [PagerDuty](https://www.pagerduty.com/) is popular at time of writing, but it can be expensive - there are other services out there like it, though. These apps should be given override ability on the engineers' phones so oncall engineers _know_ when a problem is going down and they're being paged.

## Building a Schedule

Minimum of 8 people

Perfect balance is somewhere between 12 and 25

## Big Scope or More Rotations

You will never reach perfection here. You will always have people who would prefer more rotations and people who would rather have more scope. The key is to kick a direction and stick strongly to it.

If you choose more scope, you'll be able to have a large bullpen of engineers and they will be oncall much less frequently. This will equate to less oncall burnout, engineers getting more done on feature work, and engineers not being tied to their laptops a lot. However, engineers may complain that they lose knowledge on how to perform their oncall duties given a large gap in between rotations.

In contrast, having a smaller rotation with less scope will mean that your engineers get intimately familiar with the subject matter area they're responsible for and run no risk of forgetting it. They will also experience less stress related to a wide domain they have to cover while oncall. However, your engineers will also be oncall more often which could lead to burnout and prevent them from taking vacations or really decompressing from their job.

Your rotation will likely fluctuate over time. Startups and big corporations will have to make different trade-offs.

Again, the important part is to pick a direction with your team and stick to it (at least until it becomes obvious a change is needed).

## Training Your Team

Training materials

Importance of a shadow Rotations

## Providing Support (or "How to Direct Lightning")

Develop a culture of "chip in to help oncall"

If oncall is stumped (or if they get a page in the early AM and it's a P0 page), manually page in key team members for breaking commit

Oncall should be able to pull in whatever resources they need to get a job done

## Constructing Runbooks

Everybody forgets things, so it's crucial to develop decent runbooks that are _easy_ to flip through in the heat of the moment\_

These should include at a minimum:

1. How to deploy
2. How to rollback
3. Most basic manual processes for the above
4. Leads on teams and the channels associated with those teams
